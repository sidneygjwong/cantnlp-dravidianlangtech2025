
@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {https://aclanthology.org/N18-1202/},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {2227--2237},
}

@article{behera_responsible_2023,
	title = {Responsible natural language processing: {A} principlist framework for social benefits},
	volume = {188},
	issn = {0040-1625},
	shorttitle = {Responsible natural language processing},
	url = {https://www.sciencedirect.com/science/article/pii/S0040162522008277},
	doi = {10.1016/j.techfore.2022.122306},
	abstract = {Businesses harness the power of natural language processing (NLP) to automate processes and make data-driven decisions. However, NLP raises concerns on a number of fronts due to its potential for disruption, which can be addressed with the assignment of responsibility. Therefore, responsible NLP (RNLP) can be designed as a principlist framework to ensure NLP systems are used in an ethical manner. The study proposes a principlist framework with the formulation of eight principlist ethical principles to ensure NLP is safe, secure and reliable for responsible decision making and subsequently results in social benefits. Using snowball sampling, data are collected from 15 informants, who represent senior-level positions in diversified industries. The analysis is performed with qualitative research methodology. The result produces two ethical practices. First is the adoption of RNLP as a disruptive technology for ethical decision making for social benefits and second is the creation of a culture of responsibility.},
	urldate = {2024-05-11},
	journal = {Technological Forecasting and Social Change},
	author = {Behera, Rajat Kumar and Bala, Pradip Kumar and Rana, Nripendra P. and Irani, Zahir},
	month = mar,
	year = {2023},
	pages = {122306},
}

@article{laaksonen_datafication_2020,
	title = {The {Datafication} of {Hate}: {Expectations} and {Challenges} in {Automated} {Hate} {Speech} {Monitoring}},
	volume = {3},
	issn = {2624-909X},
	shorttitle = {The {Datafication} of {Hate}},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2020.00003},
	doi = {10.3389/fdata.2020.00003},
	abstract = {Hate speech has been identified as a pressing problem in society and several automated approaches have been designed to detect and prevent it. This paper reports and reflects upon an action research setting consisting of multi-organizational collaboration conducted during Finnish municipal elections in 2017, wherein a technical infrastructure was designed to automatically monitor candidates’ social media updates for hate speech. The setting allowed us to engage in a twofold investigation. First, the collaboration offered a unique view for exploring how hate speech emerges as a technical problem. The project developed an adequately well-working algorithmic solution using supervised machine learning. We tested the performance of various feature extraction and machine learning methods and ended up using a combination of Bag-of-Words feature extraction with Support-Vector Machines. However, an automated approach required heavy simplification, such as using rudimentary scales for classifying hate speech and a reliance on word-based approaches, while in reality hate speech is a linguistic and social phenomenon with various tones and forms. Second, the action-research-oriented setting allowed us to observe affective responses, such as the hopes, dreams, and fears related to machine learning technology. Based on participatory observations, project artefacts and documents, interviews with project participants, and online reactions to the detection project, we identified participants’ aspirations for effective automation as well as the level of neutrality and objectivity introduced by an algorithmic system. However, the participants expressed more critical views towards the system after the monitoring process. Our findings highlight how the powerful expectations related to technology can easily end up dominating a project dealing with a contested, topical social issue. We conclude by discussing the problematic aspects of datafying hate and suggesting some practical implications for hate speech recognition.},
	language = {English},
	urldate = {2024-05-05},
	journal = {Frontiers in Big Data},
	author = {Laaksonen, Salla-Maaria and Haapoja, Jesse and Kinnunen, Teemu and Nelimarkka, Matti and Pöyhtäri, Reeta},
	month = feb,
	year = {2020},
}

@inproceedings{lal_g_overview_2025,
	address = {Albuquerque, NM},
	title = {Overview of the {Shared} {Task} on {Multimodal} {Hate} {Speech} {Detection} in {Dravidian} {Languages}: {DravidianLangTech}@{NAACL} 2025},
	booktitle = {Proceedings of the {Fifth} {Workshop} on {Speech}, {Vision}, and {Language} {Technologies} for {Dravidian} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Lal G, Jyothish and Premjith, B and Chakravarthi, Bharathi Raja and Rajiakodi, Saranya and B, Bharathi and Natarajan, Rajeswari and Ratnavel, Rajalakshmi},
	month = may,
	year = {2025},
}

@article{bhardwaj_investigating_2021,
	title = {Investigating {Gender} {Bias} in {BERT}},
	volume = {13},
	issn = {1866-9964},
	url = {https://doi.org/10.1007/s12559-021-09881-2},
	doi = {10.1007/s12559-021-09881-2},
	abstract = {In this work, we analyze the gender bias induced by BERT in downstream tasks. We also propose solutions to reduce gender bias. Contextual language models (CLMs) have pushed the NLP benchmarks to a new height. It has become a new norm to utilize CLM-provided word embeddings in downstream tasks such as text classification. However, unless addressed, CLMs are prone to learn intrinsic gender bias in the dataset. As a result, predictions of downstream NLP models can vary noticeably by varying gender words, such as replacing “he” to “she”, or even gender-neutral words. In this paper, we focus our analysis on a popular CLM, i.e., \$\${\textbackslash}text \{BERT\}\$\$. We analyze the gender bias it induces in five downstream tasks related to emotion and sentiment intensity prediction. For each task, we train a simple regressor utilizing \$\${\textbackslash}text \{BERT\}\$\$’s word embeddings. We then evaluate the gender bias in regressors using an equity evaluation corpus. Ideally and from the specific design, the models should discard gender informative features from the input. However, the results show a significant dependence of the system’s predictions on gender-particular words and phrases. We claim that such biases can be reduced by removing gender-specific features from word embedding. Hence, for each layer in BERT, we identify directions that primarily encode gender information. The space formed by such directions is referred to as the gender subspace in the semantic space of word embeddings. We propose an algorithm that finds fine-grained gender directions, i.e., one primary direction for each BERT layer. This obviates the need of realizing gender subspace in multiple dimensions and prevents other crucial information from being omitted. Experiments show that removing embedding components in gender directions achieves great success in reducing BERT-induced bias in the downstream tasks. The investigation reveals significant gender bias a contextualized language model ( i.e., \$\${\textbackslash}text \{BERT\}\$\$) induces in downstream tasks. The proposed solution seems promising in reducing such biases.},
	language = {en},
	number = {4},
	urldate = {2023-12-17},
	journal = {Cognitive Computation},
	author = {Bhardwaj, Rishabh and Majumder, Navonil and Poria, Soujanya},
	month = jul,
	year = {2021},
	pages = {1008--1018},
}

@inproceedings{kotek_gender_2023,
	address = {New York, NY, USA},
	series = {{CI} '23},
	title = {Gender bias and stereotypes in {Large} {Language} {Models}},
	isbn = {9798400701139},
	url = {https://dl.acm.org/doi/10.1145/3582269.3615599},
	doi = {10.1145/3582269.3615599},
	abstract = {Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs’ behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women’s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person’s gender; (b) these choices align with people’s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95\% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.},
	urldate = {2025-01-30},
	booktitle = {Proceedings of {The} {ACM} {Collective} {Intelligence} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kotek, Hadas and Dockum, Rikker and Sun, David},
	month = nov,
	year = {2023},
	pages = {12--24},
}

@inproceedings{sap_risk_2019,
	address = {Florence, Italy},
	title = {The {Risk} of {Racial} {Bias} in {Hate} {Speech} {Detection}},
	url = {https://aclanthology.org/P19-1163},
	doi = {10.18653/v1/P19-1163},
	abstract = {We investigate how annotators' insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet's dialect they are significantly less likely to label the tweet as offensive.},
	urldate = {2024-05-05},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {1668--1678},
}

@inproceedings{davidson_racial_2019,
	address = {Florence, Italy},
	title = {Racial {Bias} in {Hate} {Speech} and {Abusive} {Language} {Detection} {Datasets}},
	url = {https://aclanthology.org/W19-3504},
	doi = {10.18653/v1/W19-3504},
	abstract = {Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.},
	urldate = {2024-05-05},
	booktitle = {Proceedings of the {Third} {Workshop} on {Abusive} {Language} {Online}},
	publisher = {Association for Computational Linguistics},
	author = {Davidson, Thomas and Bhattacharya, Debasmita and Weber, Ingmar},
	editor = {Roberts, Sarah T. and Tetreault, Joel and Prabhakaran, Vinodkumar and Waseem, Zeerak},
	month = aug,
	year = {2019},
	pages = {25--35},
}

@inproceedings{lee_hate_2023,
	address = {Dubrovnik, Croatia},
	title = {Hate {Speech} {Classifiers} are {Culturally} {Insensitive}},
	url = {https://aclanthology.org/2023.c3nlp-1.5},
	doi = {10.18653/v1/2023.c3nlp-1.5},
	abstract = {Increasingly, language models and machine translation are becoming valuable tools to help people communicate with others from diverse cultural backgrounds. However, current language models lack cultural awareness because they are trained on data representing only the culture within the dataset. This presents a problem in the context of hate speech classification, where cultural awareness is especially critical. This study aims to quantify the cultural insensitivity of three monolingual (Korean, English, Arabic) hate speech classifiers by evaluating their performance on translated datasets from the other two languages. Our research has revealed that hate speech classifiers evaluated on datasets from other cultures yield significantly lower F1 scores, up to almost 50\%. In addition, they produce considerably higher false negative rates, with a magnitude up to five times greater, demonstrating the extent of the cultural gap. The study highlights the severity of cultural insensitivity of language models in hate speech classification.},
	urldate = {2024-04-22},
	booktitle = {Proceedings of the {First} {Workshop} on {Cross}-{Cultural} {Considerations} in {NLP} ({C3NLP})},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Nayeon and Jung, Chani and Oh, Alice},
	editor = {Dev, Sunipa and Prabhakaran, Vinodkumar and Adelani, David and Hovy, Dirk and Benotti, Luciana},
	month = may,
	year = {2023},
	pages = {35--46},
}

@inproceedings{wong_sociocultural_2024,
	address = {Bangkok, Thailand},
	title = {Sociocultural {Considerations} in {Monitoring} {Anti}-{LGBTQ}+ {Content} on {Social} {Media}},
	copyright = {All rights reserved},
	url = {https://aclanthology.org/2024.c3nlp-1.7},
	abstract = {The purpose of this paper is to ascertain the influence of sociocultural factors (i.e., social, cultural, and political) in the development of hate speech detection systems. We set out to investigate the suitability of using open-source training data to monitor levels of anti-LGBTQ+ content on social media across different national-varieties of English. Our findings suggests the social and cultural alignment of open-source hate speech data sets influences the predicted outputs. Furthermore, the keyword-search approach of anti-LGBTQ+ slurs in the development of open-source training data encourages detection models to overfit on slurs; therefore, anti-LGBTQ+ content may go undetected. We recommend combining empirical outputs with qualitative insights to ensure these systems are fit for purpose.},
	urldate = {2024-08-18},
	booktitle = {Proceedings of the 2nd {Workshop} on {Cross}-{Cultural} {Considerations} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wong, Sidney},
	editor = {Prabhakaran, Vinodkumar and Dev, Sunipa and Benotti, Luciana and Hershcovich, Daniel and Cabello, Laura and Cao, Yong and Adebara, Ife and Zhou, Li},
	month = aug,
	year = {2024},
	pages = {84--97},
}

@inproceedings{zhou_exploring_2019,
	address = {New York, NY, USA},
	series = {{ICMI} '19},
	title = {Exploring {Emotion} {Features} and {Fusion} {Strategies} for {Audio}-{Video} {Emotion} {Recognition}},
	isbn = {978-1-4503-6860-5},
	url = {https://doi.org/10.1145/3340555.3355713},
	doi = {10.1145/3340555.3355713},
	abstract = {The audio-video based emotion recognition aims to classify a given video into basic emotions. In this paper, we describe our approaches in EmotiW 2019, which mainly explores emotion features and feature fusion strategies for audio and visual modality. For emotion features, we explore audio feature with both speech-spectrogram and Log Mel-spectrogram and evaluate several facial features with different CNN models and different emotion pretrained strategies. For fusion strategies, we explore intra-modal and cross-modal fusion methods, such as designing attention mechanisms to highlights important emotion feature, exploring feature concatenation and factorized bilinear pooling (FBP) for cross-modal feature fusion. With careful evaluation, we obtain 65.5\% on the AFEW validation set and 62.48\% on the test set and rank third in the challenge.},
	urldate = {2025-01-30},
	booktitle = {2019 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Hengshun and Meng, Debin and Zhang, Yuanyuan and Peng, Xiaojiang and Du, Jun and Wang, Kai and Qiao, Yu},
	month = oct,
	year = {2019},
	pages = {562--566},
}

@article{stevens_scale_1937,
	title = {A {Scale} for the {Measurement} of the {Psychological} {Magnitude} {Pitch}},
	volume = {8},
	issn = {0001-4966},
	url = {https://doi.org/10.1121/1.1915893},
	doi = {10.1121/1.1915893},
	abstract = {A subjective scale for the measurement of pitch was constructed from determinations of the half‐value of pitches at various frequencies. This scale differs from both the musical scale and the frequency scale, neither of which is subjective. Five observers fractionated tones of 10 different frequencies at a loudness level of 60 db. From these fractionations a numerical scale was constructed which is proportional to the perceived magnitude of subjective pitch. In numbering the scale the 1000‐cycle tone was assigned the pitch of 1000 subjective units (mels). The close agreement of the pitch scale with an integration of the differential thresholds (DL's) shows that, unlike the DL's for loudness, all DL's for pitch are of uniform subjective magnitude. The agreement further implies that pitch and differential sensitivity to pitch are both rectilinear functions of extent on the basilar membrane. The correspondence of the pitch scale and the experimentally determined location of the resonant areas of the basilar membrane suggests that, in cutting a pitch in half, the observer adjusts the tone until it stimulates a position half‐way from the original locus to the apical end of the membrane. Measurement of the subjective size of musical intervals (such as octaves) in terms of the pitch scale shows that the intervals become larger as the frequency of the mid‐point of the interval increases (except in the two highest audible octaves). This result confirms earlier judgments as to the relative size of octaves in different parts of the frequency range.},
	number = {3},
	urldate = {2025-01-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Stevens, S. S. and Volkmann, J. and Newman, E. B.},
	month = jan,
	year = {1937},
	pages = {185--190},
}

@article{davis_comparison_1980,
	title = {Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},
	volume = {28},
	issn = {0096-3518},
	url = {https://ieeexplore.ieee.org/document/1163420},
	doi = {10.1109/TASSP.1980.1163420},
	abstract = {Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.},
	number = {4},
	urldate = {2025-01-30},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Davis, S. and Mermelstein, P.},
	month = aug,
	year = {1980},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	pages = {357--366},
}

@inproceedings{arroniz_was_2023,
	address = {Varna, Bulgaria},
	title = {Was {That} a {Question}? {Automatic} {Classification} of {Discourse} {Meaning} in {Spanish}},
	shorttitle = {Was {That} a {Question}?},
	url = {https://aclanthology.org/2023.ranlp-1.15/},
	abstract = {This paper examines the effectiveness of different feature representations of audio data in accurately classifying discourse meaning in Spanish. The task involves determining whether an utterance is a declarative sentence, an interrogative, an imperative, etc. We explore how pitch contour can be represented for a discourse-meaning classification task, employing three different audio features: MFCCs, Mel-scale spectrograms, and chromagrams. We also determine if utilizing means is more effective in representing the speech signal, given the large number of coefficients produced during the feature extraction process. Finally, we evaluate whether these feature representation techniques are sensitive to speaker information. Our results show that a recurrent neural network architecture in conjunction with all three feature sets yields the best results for the task.},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing}},
	publisher = {INCOMA Ltd., Shoumen, Bulgaria},
	author = {Arróniz, Santiago and Kübler, Sandra},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2023},
	pages = {132--142},
}

@misc{verma_towards_2024,
	title = {Towards {Signal} {Processing} {In} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.10254},
	doi = {10.48550/arXiv.2406.10254},
	abstract = {This paper introduces the idea of applying signal processing inside a Large Language Model (LLM). With the recent explosion of generative AI, our work can help bridge two fields together, namely the field of signal processing and large language models. We draw parallels between classical Fourier-Transforms and Fourier Transform-like learnable time-frequency representations for every intermediate activation signal of an LLM. Once we decompose every activation signal across tokens into a time-frequency representation, we learn how to filter and reconstruct them, with all components learned from scratch, to predict the next token given the previous context. We show that for GPT-like architectures, our work achieves faster convergence and significantly increases performance by adding a minuscule number of extra parameters when trained for the same epochs. We hope this work paves the way for algorithms exploring signal processing inside the signals found in neural architectures like LLMs and beyond.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Verma, Prateek and Pilanci, Mert},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10254 [cs]
version: 1},
}

@article{chhabra_literature_2023,
	title = {A literature survey on multimodal and multilingual automatic hate speech identification},
	volume = {29},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-023-01051-8},
	doi = {10.1007/s00530-023-01051-8},
	abstract = {Social media is a more common and powerful platform for communication to share views about any topic or article, which consequently leads to unstructured toxic, and hateful conversations. Curbing hate speeches has emerged as a critical challenge globally. In this regard, Social media platforms are using modern statistical tools of AI technologies to process and eliminate toxic data to minimize hate crimes globally. Demanding the dire need, machine and deep learning-based techniques are getting more attention in analyzing these kinds of data. This survey presents a comprehensive analysis of hate speech definitions along with the motivation for detection and standard textual analysis methods that play a crucial role in identifying hate speech. State-of-the-art hate speech identification methods are also discussed, highlighting handcrafted feature-based and deep learning-based algorithms by considering multimodal and multilingual inputs and stating the pros and cons of each. Survey also presents popular benchmark datasets of hate speech/offensive language detection specifying their challenges, the methods for achieving top classification scores, and dataset characteristics such as the number of samples, modalities, language(s), number of classes, etc. Additionally, performance metrics are described, and classification scores of popular hate speech methods are mentioned. The conclusion and future research directions are presented at the end of the survey. Compared with earlier surveys, this paper gives a better presentation of multimodal and multilingual hate speech detection through well-organized comparisons, challenges, and the latest evaluation techniques, along with their best performances.},
	language = {en},
	number = {3},
	urldate = {2025-01-29},
	journal = {Multimedia Systems},
	author = {Chhabra, Anusha and Vishwakarma, Dinesh Kumar},
	month = jun,
	year = {2023},
	pages = {1203--1230},
}

@inproceedings{kiela_hateful_2020,
	title = {The {Hateful} {Memes} {Challenge}: {Detecting} {Hate} {Speech} in {Multimodal} {Memes}},
	volume = {33},
	shorttitle = {The {Hateful} {Memes} {Challenge}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1b84c4cee2b8b3d823b30e2d604b1878-Abstract.html},
	abstract = {This work proposes a new challenge set for multimodal classification, focusing on
detecting hate speech in multimodal memes. It is constructed such that unimodal
models struggle and only multimodal models can succeed: difficult examples
(“benign confounders”) are added to the dataset to make it hard to rely on unimodal
signals. The task requires subtle reasoning, yet is straightforward to evaluate
as a binary classification problem. We provide baseline performance numbers
for unimodal models, as well as for multimodal models with various degrees of
sophistication. We find that state-of-the-art methods perform poorly compared to
humans, illustrating the difficulty of the task and highlighting the challenge that this important problem poses to the community.},
	urldate = {2025-01-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
	year = {2020},
	pages = {2611--2624},
}

@inproceedings{wong_cantnlplt-edi-2024_2024,
	address = {St. Julian's, Malta},
	title = {cantnlp@{LT}-{EDI}-2024: {Automatic} {Detection} of {Anti}-{LGBTQ}+ {Hate} {Speech} in {Under}-resourced {Languages}},
	shorttitle = {cantnlp@{LT}-{EDI}-2024},
	url = {https://aclanthology.org/2024.ltedi-1.19},
	abstract = {This paper describes our homophobia/transphobia in social media comments detection system developed as part of the shared task at LT-EDI-2024. We took a transformer-based approach to develop our multiclass classification model for ten language conditions (English, Spanish, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, Tulu, and Telugu). We introduced synthetic and organic instances of script-switched language data during domain adaptation to mirror the linguistic realities of social media language as seen in the labelled training data. Our system ranked second for Gujarati and Telugu with varying levels of performance for other language conditions. The results suggest incorporating elements of paralinguistic behaviour such as script-switching may improve the performance of language detection systems especially in the cases of under-resourced languages conditions.},
	urldate = {2024-05-17},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Language} {Technology} for {Equality}, {Diversity}, {Inclusion}},
	publisher = {Association for Computational Linguistics},
	author = {Wong, Sidney and Durward, Matthew},
	editor = {Chakravarthi, Bharathi Raja and B, Bharathi and Buitelaar, Paul and Durairaj, Thenmozhi and Kovács, György and García Cumbreras, Miguel Ángel},
	month = mar,
	year = {2024},
	pages = {177--183},
}

@inproceedings{chakravarthi_findings_2021,
	address = {Kyiv},
	title = {Findings of the {Shared} {Task} on {Offensive} {Language} {Identification} in {Tamil}, {Malayalam}, and {Kannada}},
	url = {https://aclanthology.org/2021.dravidianlangtech-1.17/},
	abstract = {Detecting offensive language in social media in local languages is critical for moderating user-generated content. Thus, the field of offensive language identification in under-resourced Tamil, Malayalam and Kannada languages are essential. As the user-generated content is more code-mixed and not well studied for under-resourced languages, it is imperative to create resources and conduct benchmarking studies to encourage research in under-resourced Dravidian languages. We created a shared task on offensive language detection in Dravidian languages. We summarize here the dataset for this challenge which are openly available at https://competitions.codalab.org/competitions/27654, and present an overview of the methods and the results of the competing systems.},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the {First} {Workshop} on {Speech} and {Language} {Technologies} for {Dravidian} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Jose, Navya and Kumar M, Anand and Mandl, Thomas and Kumaresan, Prasanna Kumar and Ponnusamy, Rahul and R L, Hariharan and McCrae, John P. and Sherly, Elizabeth},
	editor = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Kumar M, Anand and Krishnamurthy, Parameswari and Sherly, Elizabeth},
	month = apr,
	year = {2021},
	pages = {133--145},
}

@inproceedings{yasaswini_iiittdravidianlangtech-eacl2021_2021,
	address = {Kyiv},
	title = {{IIITT}@{DravidianLangTech}-{EACL2021}: {Transfer} {Learning} for {Offensive} {Language} {Detection} in {Dravidian} {Languages}},
	shorttitle = {{IIITT}@{DravidianLangTech}-{EACL2021}},
	url = {https://aclanthology.org/2021.dravidianlangtech-1.25/},
	abstract = {This paper demonstrates our work for the shared task on Offensive Language Identification in Dravidian Languages-EACL 2021. Offensive language detection in the various social media platforms was identified previously. But with the increase in diversity of users, there is a need to identify the offensive language in multilingual posts that are largely code-mixed or written in a non-native script. We approach this challenge with various transfer learning-based models to classify a given post or comment in Dravidian languages (Malayalam, Tamil, and Kannada) into 6 categories. The source codes for our systems are published.},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the {First} {Workshop} on {Speech} and {Language} {Technologies} for {Dravidian} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Yasaswini, Konthala and Puranik, Karthik and Hande, Adeep and Priyadharshini, Ruba and Thavareesan, Sajeetha and Chakravarthi, Bharathi Raja},
	editor = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Kumar M, Anand and Krishnamurthy, Parameswari and Sherly, Elizabeth},
	month = apr,
	year = {2021},
	pages = {187--194},
}

@misc{khanuja_muril_2021,
	title = {{MuRIL}: {Multilingual} {Representations} for {Indian} {Languages}},
	shorttitle = {{MuRIL}},
	url = {http://arxiv.org/abs/2103.10730},
	doi = {10.48550/arXiv.2103.10730},
	abstract = {India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011). India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today's state-of-the-art multilingual systems perform suboptimally on Indian (IN) languages. This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn't help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both translated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native to Latin script) test sets of the chosen datasets and demonstrate the efficacy of MuRIL in handling transliterated data.},
	urldate = {2023-09-01},
	publisher = {arXiv},
	author = {Khanuja, Simran and Bansal, Diksha and Mehtani, Sarvesh and Khosla, Savya and Dey, Atreyee and Gopalan, Balaji and Margam, Dilip Kumar and Aggarwal, Pooja and Nagipogu, Rajiv Teja and Dave, Shachi and Gupta, Shruti and Gali, Subhash Chandra Bose and Subramanian, Vish and Talukdar, Partha},
	month = apr,
	year = {2021},
	note = {arXiv:2103.10730 [cs]},
}

@inproceedings{conneau_unsupervised_2020,
	address = {Online},
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {https://aclanthology.org/2020.acl-main.747},
	doi = {10.18653/v1/2020.acl-main.747},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.},
	urldate = {2024-05-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {8440--8451},
}

@inproceedings{chakravarthi_overview_2024,
	address = {St. Julian's, Malta},
	title = {Overview of {Third} {Shared} {Task} on {Homophobia} and {Transphobia} {Detection} in {Social} {Media} {Comments}},
	url = {https://aclanthology.org/2024.ltedi-1.11},
	abstract = {This paper provides a comprehensive summary of the “Homophobia and Transphobia Detection in Social Media Comments” shared task, which was held at the LT-EDI@EACL 2024. The objective of this task was to develop systems capable of identifying instances of homophobia and transphobia within social media comments. This challenge was extended across ten languages: English, Tamil, Malayalam, Telugu, Kannada, Gujarati, Hindi, Marathi, Spanish, and Tulu. Each comment in the dataset was annotated into three categories. The shared task attracted significant interest, with over 60 teams participating through the CodaLab platform. The submission of prediction from the participants was evaluated with the macro F1 score.},
	urldate = {2024-04-23},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Language} {Technology} for {Equality}, {Diversity}, {Inclusion}},
	publisher = {Association for Computational Linguistics},
	author = {Chakravarthi, Bharathi Raja and Kumaresan, Prasanna and Priyadharshini, Ruba and Buitelaar, Paul and Hegde, Asha and Shashirekha, Hosahalli and Rajiakodi, Saranya and García, Miguel Ángel and Jiménez-Zafra, Salud María and García-Díaz, José and Valencia-García, Rafael and Ponnusamy, Kishore and Shetty, Poorvi and García-Baena, Daniel},
	editor = {Chakravarthi, Bharathi Raja and B, Bharathi and Buitelaar, Paul and Durairaj, Thenmozhi and Kovács, György and García Cumbreras, Miguel Ángel},
	month = mar,
	year = {2024},
	pages = {124--132},
}

@inproceedings{chakravarthi_overview_2022,
	address = {Dublin, Ireland},
	title = {Overview of {The} {Shared} {Task} on {Homophobia} and {Transphobia} {Detection} in {Social} {Media} {Comments}},
	url = {https://aclanthology.org/2022.ltedi-1.57},
	doi = {10.18653/v1/2022.ltedi-1.57},
	abstract = {Homophobia and Transphobia Detection is the task of identifying homophobia, transphobia, and non-anti-LGBT+ content from the given corpus. Homophobia and transphobia are both toxic languages directed at LGBTQ+ individuals that are described as hate speech. This paper summarizes our findings on the “Homophobia and Transphobia Detection in social media comments” shared task held at LT-EDI 2022 - ACL 2022 1. This shared taskfocused on three sub-tasks for Tamil, English, and Tamil-English (code-mixed) languages. It received 10 systems for Tamil, 13 systems for English, and 11 systems for Tamil-English. The best systems for Tamil, English, and Tamil-English scored 0.570, 0.870, and 0.610, respectively, on average macro F1-score.},
	urldate = {2023-06-05},
	booktitle = {Proceedings of the {Second} {Workshop} on {Language} {Technology} for {Equality}, {Diversity} and {Inclusion}},
	publisher = {Association for Computational Linguistics},
	author = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Durairaj, Thenmozhi and McCrae, John and Buitelaar, Paul and Kumaresan, Prasanna and Ponnusamy, Rahul},
	month = may,
	year = {2022},
	pages = {369--377},
}

@inproceedings{parra_escartin_ethical_2017,
	address = {Valencia, Spain},
	title = {Ethical {Considerations} in {NLP} {Shared} {Tasks}},
	url = {https://aclanthology.org/W17-1608},
	doi = {10.18653/v1/W17-1608},
	abstract = {Shared tasks are increasingly common in our field, and new challenges are suggested at almost every conference and workshop. However, as this has become an established way of pushing research forward, it is important to discuss how we researchers organise and participate in shared tasks, and make that information available to the community to allow further research improvements. In this paper, we present a number of ethical issues along with other areas of concern that are related to the competitive nature of shared tasks. As such issues could potentially impact on research ethics in the Natural Language Processing community, we also propose the development of a framework for the organisation of and participation in shared tasks that can help mitigate against these issues arising.},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Parra Escartín, Carla and Reijers, Wessel and Lynn, Teresa and Moorkens, Joss and Way, Andy and Liu, Chao-Hong},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {66--73},
}

@inproceedings{hovy_social_2016,
	address = {Berlin, Germany},
	title = {The {Social} {Impact} of {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/P16-2096},
	doi = {10.18653/v1/P16-2096},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Spruit, Shannon L.},
	editor = {Erk, Katrin and Smith, Noah A.},
	month = aug,
	year = {2016},
	pages = {591--598},
}

@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423.pdf?utm_medium=email&utm_source=transaction},
	urldate = {2024-05-06},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}

@article{jahan_systematic_2023,
	title = {A systematic review of hate speech automatic detection using natural language processing},
	volume = {546},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223003557},
	doi = {10.1016/j.neucom.2023.126232},
	abstract = {With the multiplication of social media platforms, which offer anonymity, easy access and online community formation and online debate, the issue of hate speech detection and tracking becomes a growing challenge to society, individual, policy-makers and researchers. Despite efforts for leveraging automatic techniques for automatic detection and monitoring, their performances are still far from satisfactory, which constantly calls for future research on the issue. This paper provides a systematic review of literature in this field, with a focus on natural language processing and deep learning technologies, highlighting the terminology, processing pipeline, core methods employed, with a focal point on deep learning architecture. From a methodological perspective, we adopt PRISMA guideline of systematic review of the last 10 years literature from ACM Digital Library and Google Scholar. In the sequel, existing surveys, limitations, and future research directions are extensively discussed.},
	urldate = {2024-04-23},
	journal = {Neurocomputing},
	author = {Jahan, Md Saroar and Oussalah, Mourad},
	month = aug,
	year = {2023},
	pages = {126232},
}

@inproceedings{xiang_detecting_2012,
	address = {New York, NY, USA},
	series = {{CIKM} '12},
	title = {Detecting offensive tweets via topical feature discovery over a large scale twitter corpus},
	isbn = {978-1-4503-1156-4},
	url = {https://doi.org/10.1145/2396761.2398556},
	doi = {10.1145/2396761.2398556},
	abstract = {In this paper, we propose a novel semi-supervised approach for detecting profanity-related offensive content in Twitter. Our approach exploits linguistic regularities in profane language via statistical topic modeling on a huge Twitter corpus, and detects offensive tweets using automatically these generated features. Our approach performs competitively with a variety of machine learning (ML) algorithms. For instance, our approach achieves a true positive rate (TP) of 75.1\% over 4029 testing tweets using Logistic Regression, significantly outperforming the popular keyword matching baseline, which has a TP of 69.7\%, while keeping the false positive rate (FP) at the same level as the baseline at about 3.77\%. Our approach provides an alternative to large scale hand annotation efforts required by fully supervised learning approaches.},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the 21st {ACM} international conference on {Information} and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Xiang, Guang and Fan, Bin and Wang, Ling and Hong, Jason and Rose, Carolyn},
	month = oct,
	year = {2012},
	pages = {1980--1984},
}

@article{dinakar_common_2012,
	title = {Common {Sense} {Reasoning} for {Detection}, {Prevention}, and {Mitigation} of {Cyberbullying}},
	volume = {2},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/2362394.2362400},
	doi = {10.1145/2362394.2362400},
	abstract = {Cyberbullying (harassment on social networks) is widely recognized as a serious social problem, especially for adolescents. It is as much a threat to the viability of online social networks for youth today as spam once was to email in the early days of the Internet. Current work to tackle this problem has involved social and psychological studies on its prevalence as well as its negative effects on adolescents. While true solutions rest on teaching youth to have healthy personal relationships, few have considered innovative design of social network software as a tool for mitigating this problem. Mitigating cyberbullying involves two key components: robust techniques for effective detection and reflective user interfaces that encourage users to reflect upon their behavior and their choices.Spam filters have been successful by applying statistical approaches like Bayesian networks and hidden Markov models. They can, like Google’s GMail, aggregate human spam judgments because spam is sent nearly identically to many people. Bullying is more personalized, varied, and contextual. In this work, we present an approach for bullying detection based on state-of-the-art natural language processing and a common sense knowledge base, which permits recognition over a broad spectrum of topics in everyday life. We analyze a more narrow range of particular subject matter associated with bullying (e.g. appearance, intelligence, racial and ethnic slurs, social acceptance, and rejection), and construct BullySpace, a common sense knowledge base that encodes particular knowledge about bullying situations. We then perform joint reasoning with common sense knowledge about a wide range of everyday life topics. We analyze messages using our novel AnalogySpace common sense reasoning technique. We also take into account social network analysis and other factors. We evaluate the model on real-world instances that have been reported by users on Formspring, a social networking website that is popular with teenagers.On the intervention side, we explore a set of reflective user-interaction paradigms with the goal of promoting empathy among social network participants. We propose an “air traffic control”-like dashboard, which alerts moderators to large-scale outbreaks that appear to be escalating or spreading and helps them prioritize the current deluge of user complaints. For potential victims, we provide educational material that informs them about how to cope with the situation, and connects them with emotional support from others. A user evaluation shows that in-context, targeted, and dynamic help during cyberbullying situations fosters end-user reflection that promotes better coping strategies.},
	number = {3},
	urldate = {2025-01-28},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Dinakar, Karthik and Jones, Birago and Havasi, Catherine and Lieberman, Henry and Picard, Rosalind},
	month = sep,
	year = {2012},
	pages = {18:1--18:30},
}

@inproceedings{warner_detecting_2012,
	address = {USA},
	series = {{LSM} '12},
	title = {Detecting hate speech on the world wide web},
	abstract = {We present an approach to detecting hate speech in online text, where hate speech is defined as abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation. While hate speech against any group may exhibit some common characteristics, we have observed that hatred against each different group is typically characterized by the use of a small set of high frequency stereotypical words; however, such words may be used in either a positive or a negative sense, making our task similar to that of words sense disambiguation. In this paper we describe our definition of hate speech, the collection and annotation of our hate speech corpus, and a mechanism for detecting some commonly used methods of evading common "dirty word" filters. We describe pilot classification experiments in which we classify anti-semitic speech reaching an accuracy 94\%, precision of 68\% and recall at 60\%, for an F1 measure of. 6375.},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the {Second} {Workshop} on {Language} in {Social} {Media}},
	publisher = {Association for Computational Linguistics},
	author = {Warner, William and Hirschberg, Julia},
	month = jun,
	year = {2012},
	pages = {19--26},
}

@inproceedings{chen_detecting_2012,
	title = {Detecting {Offensive} {Language} in {Social} {Media} to {Protect} {Adolescent} {Online} {Safety}},
	url = {https://ieeexplore.ieee.org/abstract/document/6406271},
	doi = {10.1109/SocialCom-PASSAT.2012.55},
	abstract = {Since the textual contents on online social media are highly unstructured, informal, and often misspelled, existing research on message-level offensive language detection cannot accurately detect offensive content. Meanwhile, user-level offensiveness detection seems a more feasible approach but it is an under researched area. To bridge this gap, we propose the Lexical Syntactic Feature (LSF) architecture to detect offensive content and identify potential offensive users in social media. We distinguish the contribution of pejoratives/profanities and obscenities in determining offensive content, and introduce hand-authoring syntactic rules in identifying name-calling harassments. In particular, we incorporate a user's writing style, structure and specific cyber bullying content as features to predict the user's potentiality to send out offensive content. Results from experiments showed that our LSF framework performed significantly better than existing methods in offensive content detection. It achieves precision of 98.24\% and recall of 94.34\% in sentence offensive detection, as well as precision of 77.9\% and recall of 77.8\% in user offensive detection. Meanwhile, the processing speed of LSF is approximately 10msec per sentence, suggesting the potential for effective deployment in social media.},
	urldate = {2025-01-28},
	booktitle = {2012 {International} {Conference} on {Privacy}, {Security}, {Risk} and {Trust} and 2012 {International} {Confernece} on {Social} {Computing}},
	author = {Chen, Ying and Zhou, Yilu and Zhu, Sencun and Xu, Heng},
	month = sep,
	year = {2012},
	pages = {71--80},
}

@article{rawat_hate_2024,
	title = {Hate speech detection in social media: {Techniques}, recent trends, and future challenges},
	volume = {16},
	issn = {1939-0068},
	shorttitle = {Hate speech detection in social media},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1648},
	doi = {10.1002/wics.1648},
	abstract = {The realm of Natural Language Processing and Text Mining has seen a surge in interest from researchers in hate speech detection, leading to an increase in related studies. This analysis aims to create a valuable resource by summarizing the methods and strategies used to combat hate speech in social media. We perform a detailed review to achieve a deep knowledge of the hate speech detection landscape from 2018 to 2023, revealing global incidents of hate speech in 2022–2023. Sixty-six relevant articles were selected for this review. Existing studies were analyzed and categorized into five method categories: Machine Learning, Deep Learning, Ensemble models, Graph Neural Networks, and Graph Convolutional Networks. These advancements can aid social networking services in identifying hate messages before being posted, reducing the risk of harassment. The review also covers available hate speech datasets and highlights research challenges, but it is clear that a definitive solution to this problem is yet to be found. Future research directions are recommended to address the ongoing challenges in Hate Speech Detection. This article is categorized under: Applications of Computational Statistics {\textgreater} Computational Linguistics Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Knowledge Discovery Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Classification and Regression Trees (CART) Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Text Mining},
	language = {en},
	number = {2},
	urldate = {2024-05-16},
	journal = {WIREs Computational Statistics},
	author = {Rawat, Anchal and Kumar, Santosh and Samant, Surender Singh},
	year = {2024},
	pages = {e1648},
}

@inproceedings{s_wit_2024,
	address = {St. Julian's, Malta},
	title = {Wit {Hub}@{DravidianLangTech}-2024:{Multimodal} {Social} {Media} {Data} {Analysis} in {Dravidian} {Languages} using {Machine} {Learning} {Models}},
	shorttitle = {Wit {Hub}@{DravidianLangTech}-2024},
	url = {https://aclanthology.org/2024.dravidianlangtech-1.38/},
	abstract = {The main objective of the task is categorised into three subtasks. Subtask-1 Build models to determine the sentiment expressed in multimodal posts (or videos) in Tamil and Malayalam languages, leveraging textual, audio, and visual components. The videos are labelled into five categories: highly positive, positive, neutral, negative and highly negative. Subtask-2 Design machine models that effectively identify and classify abusive language within the multimodal context of social media posts in Tamil. The data are categorized into abusive and non-abusive categories. Subtask-3 Develop advanced models that accurately detect and categorize hate speech and offensive language in multimodal social media posts in Dravidian languages. The data points are categorized into Caste, Offensive, Racist and Sexist classes. In this session, the focus is primarily on Tamil language text data analysis. Various combination of machine learning models have been used to perform each tasks and do oversampling techniques to train models on biased dataset.},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Speech}, {Vision}, and {Language} {Technologies} for {Dravidian} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {S, Anierudh and R, Abhishek and Sundar, Ashwin and Krishnan, Amrit and B, Bharathi},
	editor = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Madasamy, Anand Kumar and Thavareesan, Sajeetha and Sherly, Elizabeth and Nadarajan, Rajeswari and Ravikiran, Manikandan},
	month = mar,
	year = {2024},
	pages = {229--233},
}

@inproceedings{rahman_binary_beastsdravidianlangtech-eacl_2024,
	address = {St. Julian's, Malta},
	title = {Binary\_Beasts@{DravidianLangTech}-{EACL} 2024: {Multimodal} {Abusive} {Language} {Detection} in {Tamil} based on {Integrated} {Approach} of {Machine} {Learning} and {Deep} {Learning} {Techniques}},
	shorttitle = {Binary\_Beasts@{DravidianLangTech}-{EACL} 2024},
	url = {https://aclanthology.org/2024.dravidianlangtech-1.35/},
	abstract = {Detecting abusive language on social media is a challenging task that needs to be solved effectively. This research addresses the formidable challenge of detecting abusive language in Tamil through a comprehensive multimodal approach, incorporating textual, acoustic, and visual inputs. This study utilized ConvLSTM, 3D-CNN, and a hybrid 3D-CNN with BiLSTM to extract video features. Several models, such as BiLSTM, LR, and CNN, are explored for processing audio data, whereas for textual content, MNB, LR, and LSTM methods are explored. To further enhance overall performance, this work introduced a weighted late fusion model amalgamating predictions from all modalities. The fusion model was then applied to make predictions on the test dataset. The ConvLSTM+BiLSTM+MNB model yielded the highest macro F1 score of 71.43\%. Our methodology allowed us to achieve 1 st rank for multimodal abusive language detection in the shared task},
	urldate = {2025-01-27},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Speech}, {Vision}, and {Language} {Technologies} for {Dravidian} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Rahman, Md. and Raihan, Abu and Rahman, Tanzim and Ahsan, Shawly and Hossain, Jawad and Das, Avishek and Hoque, Mohammed Moshiul},
	editor = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Madasamy, Anand Kumar and Thavareesan, Sajeetha and Sherly, Elizabeth and Nadarajan, Rajeswari and Ravikiran, Manikandan},
	month = mar,
	year = {2024},
	pages = {212--217},
}

@inproceedings{chakravarthi_findings_2024,
	address = {St. Julian's, Malta},
	title = {Findings of the {Shared} {Task} on {Multimodal} {Social} {Media} {Data} {Analysis} in {Dravidian} {Languages} ({MSMDA}-{DL})@{DravidianLangTech} 2024},
	url = {https://aclanthology.org/2024.dravidianlangtech-1.9/},
	abstract = {This paper presents the findings of the shared task on multimodal sentiment analysis, abusive language detection and hate speech detection in Dravidian languages. Through this shared task, researchers worldwide can submit models for three crucial social media data analysis challenges in Dravidian languages: sentiment analysis, abusive language detection, and hate speech detection. The aim is to build models for deriving fine-grained sentiment analysis from multimodal data in Tamil and Malayalam, identifying abusive and hate content from multimodal data in Tamil. Three modalities make up the multimodal data: text, audio, and video. YouTube videos were gathered to create the datasets for the tasks. Thirty-nine teams took part in the competition. However, only two teams, though, turned in their findings. The macro F1-score was used to assess the submissions},
	urldate = {2025-01-20},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Speech}, {Vision}, and {Language} {Technologies} for {Dravidian} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Madasamy, Anand Kumar and Thavareesan, Sajeetha and Sherly, Elizabeth and Nadarajan, Rajeswari and Ravikiran, Manikandan},
	month = mar,
	year = {2024},
	pages = {56--61},
}

@article{sreelakshmi_detection_2024,
	title = {Detection of {Hate} {Speech} and {Offensive} {Language} {CodeMix} {Text} in {Dravidian} {Languages} {Using} {Cost}-{Sensitive} {Learning} {Approach}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/10419328},
	doi = {10.1109/ACCESS.2024.3358811},
	abstract = {Recently, the emergence of social media has opened the way for online harassment in the form of hate speech and offensive language. An automated approach is needed to detect hate and offensive content from social media, which is indispensable. This task is challenging in the case of social media posts or comments in low-resourced CodeMix languages. This paper investigates the efficacy of various multilingual transformer-based embedding models with machine learning classifiers for detecting hate speech and offensive language (HOS) content in social media posts in CodeMix Dravidian languages that belong to the low-resource language group. Experiments were conducted on six sets of openly available datasets in Kannada-English, Malayalam-English and Tamil-English languages. The objective is to identify a single pre-trained embedding model that commonly works well for HOS tasks in the above mentioned languages. For this, a comprehensive study of various multilingual transformer embedding models, such as BERT, DistilBERT, LaBSE, MuRIL, XLM, IndicBERT, and FNET for HOS detection was conducted. Our experiments revealed that MuRIL pre-trained embedding performed consistently well for all six datasets using Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel. In a set of experiments conducted on six datasets, the highest accuracy results for each dataset are as follows: DravidianLangTech 2021 achieved 96\% accuracy for Malayalam, 72\% accuracy for Tamil, and 66\% accuracy for Kannada. For HASOC 2021 Tamil, the accuracy reached 76\%, and for HASOC 2021 Malayalam, it reached 68\%. Additionally, HASOC 2020 demonstrated an accuracy of 92\% for Malayalam. Moreover, we performed an in-depth error analysis and a comparative study, presenting a tabulated summary of our work compared to other top-performing studies. In addition, we employed a cost-sensitive learning approach to address the class imbalance problem in the dataset, in which minority classes get higher classification weights than the majority classes. The weights were initialized and fine-tuned to obtain the best balance between all the classes. The results showed that incorporating the cost-sensitive learning strategy avoided class bias in the trained model. In addition to the aforementioned points, a significant contribution of our research presented in this paper is introducing a novel annotated test set for Malayalam-English CodeMix. This new dataset serves as an extension to our existing data, known as the Hate Speech and Offensive Content Identification in English and Indo-Aryan Languages (HASOC) 2021 Malayalam-English dataset.},
	urldate = {2025-01-20},
	journal = {IEEE Access},
	author = {Sreelakshmi, K. and Premjith, B. and Chakravarthi, Bharathi Raja and Soman, K. P.},
	year = {2024},
	note = {Conference Name: IEEE Access},
	pages = {20064--20090},
}

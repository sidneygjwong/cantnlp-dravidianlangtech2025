{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhEcf0skNuUXKiV0atxt/Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Initialisation"],"metadata":{"id":"1XZ53sWmUouQ"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAe9M577Um-k","executionInfo":{"status":"ok","timestamp":1738188055861,"user_tz":360,"elapsed":28972,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"a6f554ca-105f-4538-83e4-ec8f216a639e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Set Working Directory\n","%cd /content/drive/MyDrive/Projects/sharedtask-dravidianlangtech"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W4QfWEQnUstt","executionInfo":{"status":"ok","timestamp":1738188065495,"user_tz":360,"elapsed":150,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"adc22427-66f7-4002-c775-96cff2f0a151"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/sharedtask-dravidianlangtech\n"]}]},{"cell_type":"markdown","source":["## Class Labels"],"metadata":{"id":"tEz4CwScFUNz"}},{"cell_type":"code","source":["def read_text(lang):\n","\n","  import pandas as pd\n","  import os\n","  import numpy as np\n","  from numpy import random\n","\n","  path = f\"!data/train/{lang}/text\"\n","  file = f\"{path}/{os.listdir(path)[0]}\"\n","\n","  data = pd.read_excel(open(file,'rb'))\n","\n","  if lang == \"telugu\":\n","    data[['label', 'details']] = data['File_Name'].str.split('_', n=1, expand=True)\n","  else:\n","    data[['label', 'details']] = data['File Name'].str.split('_', n=1, expand=True)\n","\n","  data = data.sort_values('label')\n","\n","  print(lang)\n","  print(data.groupby(['Class Label Short']).size())\n","  print(data.groupby(['label']).size())"],"metadata":{"id":"YyH0UcwkFV3Q","executionInfo":{"status":"ok","timestamp":1738188283026,"user_tz":360,"elapsed":176,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["read_text(\"malayalam\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m5Pg4IXeFZ9y","executionInfo":{"status":"ok","timestamp":1738188284872,"user_tz":360,"elapsed":307,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"2ad039cb-9288-45b3-d3f9-ee91d6c03120"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["malayalam\n","Class Label Short\n","C    186\n","G     82\n","N    406\n","P    118\n","R     91\n","dtype: int64\n","label\n","H     477\n","NH    406\n","dtype: int64\n"]}]},{"cell_type":"code","source":["read_text(\"tamil\")"],"metadata":{"id":"NQt_3s-aGHMt","executionInfo":{"status":"ok","timestamp":1738188294525,"user_tz":360,"elapsed":731,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"56adf67e-15a7-4e1d-9a72-288b20346c45","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tamil\n","Class Label Short\n","C     65\n","G     68\n","N    287\n","P     33\n","R     61\n","dtype: int64\n","label\n","H     227\n","NH    287\n","dtype: int64\n"]}]},{"cell_type":"code","source":["read_text(\"telugu\")"],"metadata":{"id":"DexIdD3RGI9r","executionInfo":{"status":"ok","timestamp":1738188299963,"user_tz":360,"elapsed":741,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"5442af0c-91d3-447c-e37f-44a00eac46ac","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["telugu\n","Class Label Short\n","C    122\n","G    106\n","N    198\n","P     58\n","R     72\n","dtype: int64\n","label\n","H     358\n","NH    198\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["## Text"],"metadata":{"id":"wn37HyIbU9rV"}},{"cell_type":"code","source":["def read_text(lang):\n","\n","  import pandas as pd\n","  import os\n","  import numpy as np\n","  from numpy import random\n","  import gensim\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","  from sklearn.metrics import accuracy_score, confusion_matrix\n","  from sklearn.metrics import classification_report\n","\n","  path = f\"!data/train/{lang}/text\"\n","  file = f\"{path}/{os.listdir(path)[0]}\"\n","\n","  data = pd.read_excel(open(file,'rb'))\n","\n","  if lang == \"telugu\":\n","    data[['label', 'details']] = data['File_Name'].str.split('_', n=1, expand=True)\n","  else:\n","    data[['label', 'details']] = data['File Name'].str.split('_', n=1, expand=True)\n","\n","  data = data.sort_values('label')\n","\n","  # Create train and test sets\n","  X = data.Transcript\n","  y = data['label']\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)\n","\n","  my_tags = list(set(y))\n","\n","  # Naive Bayes Classifier for Multinomial Models\n","  from sklearn.naive_bayes import MultinomialNB\n","  from sklearn.pipeline import Pipeline\n","  from sklearn.feature_extraction.text import TfidfTransformer\n","\n","  nb = Pipeline([('vect', CountVectorizer()),\n","                ('tfidf', TfidfTransformer()),\n","                ('clf', MultinomialNB()),\n","                ])\n","  nb.fit(X_train, y_train)\n","  nb_pred = nb.predict(X_test)\n","  print(\"Naive Bayes Classifier for Multinomial Models\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, nb_pred,target_names=my_tags,zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  # Linear Support Vector Machine\n","  from sklearn.linear_model import SGDClassifier\n","\n","  sgd = Pipeline([('vect', CountVectorizer()),\n","                  ('tfidf', TfidfTransformer()),\n","                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n","                ])\n","  sgd.fit(X_train, y_train)\n","  sgd_pred = sgd.predict(X_test)\n","  print(\"Linear Support Vector Machine\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, sgd_pred, target_names=my_tags,zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  # Logistic Regression\n","  from sklearn.linear_model import LogisticRegression\n","\n","  logreg = Pipeline([('vect', CountVectorizer()),\n","                  ('tfidf', TfidfTransformer()),\n","                  ('clf', LogisticRegression(n_jobs=2, C=1e5)),\n","                ])\n","  logreg.fit(X_train, y_train)\n","  logreg_pred = logreg.predict(X_test)\n","  print(\"Logistic Regression\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, logreg_pred,target_names=my_tags,zero_division=np.nan))\n","\n","  # Random Forest Classifier\n","  from sklearn.ensemble import RandomForestClassifier\n","\n","  forest = Pipeline([('vect', CountVectorizer()),\n","                  ('tfidf', TfidfTransformer()),\n","                  ('clf', RandomForestClassifier(max_depth=24, random_state=42)),\n","                ])\n","  forest.fit(X_train, y_train)\n","  forest_pred = forest.predict(X_test)\n","  print(\"Random Forest Classifier\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, forest_pred,target_names=my_tags,zero_division=np.nan))"],"metadata":{"id":"4wxyqMPpU9Zj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_text(\"malayalam\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zP42Ej1U9Vj","executionInfo":{"status":"ok","timestamp":1736529001680,"user_tz":300,"elapsed":759,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"3d90f6bf-1cb7-45ad-de16-02cebb53a069"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.90      0.87      0.89       125\n","           H       0.84      0.88      0.86        96\n","\n","    accuracy                           0.87       221\n","   macro avg       0.87      0.87      0.87       221\n","weighted avg       0.87      0.87      0.87       221\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.90      0.84      0.87       125\n","           H       0.81      0.88      0.84        96\n","\n","    accuracy                           0.86       221\n","   macro avg       0.85      0.86      0.85       221\n","weighted avg       0.86      0.86      0.86       221\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.86      0.86      0.86       125\n","           H       0.81      0.82      0.82        96\n","\n","    accuracy                           0.84       221\n","   macro avg       0.84      0.84      0.84       221\n","weighted avg       0.84      0.84      0.84       221\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.85      0.89      0.87       125\n","           H       0.84      0.79      0.82        96\n","\n","    accuracy                           0.85       221\n","   macro avg       0.85      0.84      0.84       221\n","weighted avg       0.85      0.85      0.85       221\n","\n"]}]},{"cell_type":"code","source":["read_text(\"tamil\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rvl3oclResQQ","executionInfo":{"status":"ok","timestamp":1736529008964,"user_tz":300,"elapsed":988,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"6c7c36e9-79fa-4bd2-aed1-a83c3997e63c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.94      0.48      0.64        60\n","           H       0.68      0.97      0.80        69\n","\n","    accuracy                           0.74       129\n","   macro avg       0.81      0.73      0.72       129\n","weighted avg       0.80      0.74      0.73       129\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.78      0.77      0.77        60\n","           H       0.80      0.81      0.81        69\n","\n","    accuracy                           0.79       129\n","   macro avg       0.79      0.79      0.79       129\n","weighted avg       0.79      0.79      0.79       129\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.76      0.70      0.73        60\n","           H       0.76      0.81      0.78        69\n","\n","    accuracy                           0.76       129\n","   macro avg       0.76      0.76      0.76       129\n","weighted avg       0.76      0.76      0.76       129\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.79      0.57      0.66        60\n","           H       0.70      0.87      0.77        69\n","\n","    accuracy                           0.73       129\n","   macro avg       0.74      0.72      0.72       129\n","weighted avg       0.74      0.73      0.72       129\n","\n"]}]},{"cell_type":"code","source":["read_text(\"telugu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5h1teuh2exoz","executionInfo":{"status":"ok","timestamp":1736529015822,"user_tz":300,"elapsed":983,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"5abff673-4ab1-4246-cb81-7fb34039842f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.74      0.96      0.84        94\n","           H       0.78      0.31      0.44        45\n","\n","    accuracy                           0.75       139\n","   macro avg       0.76      0.63      0.64       139\n","weighted avg       0.75      0.75      0.71       139\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.78      0.79      0.78        94\n","           H       0.55      0.53      0.54        45\n","\n","    accuracy                           0.71       139\n","   macro avg       0.66      0.66      0.66       139\n","weighted avg       0.70      0.71      0.70       139\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.81      0.79      0.80        94\n","           H       0.58      0.62      0.60        45\n","\n","    accuracy                           0.73       139\n","   macro avg       0.70      0.70      0.70       139\n","weighted avg       0.74      0.73      0.74       139\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.79      0.94      0.85        94\n","           H       0.78      0.47      0.58        45\n","\n","    accuracy                           0.78       139\n","   macro avg       0.78      0.70      0.72       139\n","weighted avg       0.78      0.78      0.77       139\n","\n"]}]},{"cell_type":"code","source":["def read_text(lang):\n","\n","  import pandas as pd\n","  import os\n","  import numpy as np\n","  from numpy import random\n","  import gensim\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","  from sklearn.metrics import accuracy_score, confusion_matrix\n","  from sklearn.metrics import classification_report\n","\n","  path = f\"!data/train/{lang}/text\"\n","  file = f\"{path}/{os.listdir(path)[0]}\"\n","\n","  data = pd.read_excel(open(file,'rb'))\n","\n","  if lang == \"telugu\":\n","    data[['label', 'language', 'number', 'class_label', 'details']] = data['File_Name'].str.split('_', n=4, expand=True)\n","  else:\n","    data[['label', 'language', 'number', 'class_label', 'details']] = data['File Name'].str.split('_', n=4, expand=True)\n","\n","  data = data.sort_values('class_label')\n","\n","  # Create train and test sets\n","  X = data.Transcript\n","  y = data['class_label']\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)\n","\n","  my_tags = list(set(y))\n","\n","  # Naive Bayes Classifier for Multinomial Models\n","  from sklearn.naive_bayes import MultinomialNB\n","  from sklearn.pipeline import Pipeline\n","  from sklearn.feature_extraction.text import TfidfTransformer\n","\n","  nb = Pipeline([('vect', CountVectorizer()),\n","                ('tfidf', TfidfTransformer()),\n","                ('clf', MultinomialNB()),\n","                ])\n","  nb.fit(X_train, y_train)\n","  nb_pred = nb.predict(X_test)\n","  print(\"Naive Bayes Classifier for Multinomial Models\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, nb_pred,target_names=my_tags,zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  # Linear Support Vector Machine\n","  from sklearn.linear_model import SGDClassifier\n","\n","  sgd = Pipeline([('vect', CountVectorizer()),\n","                  ('tfidf', TfidfTransformer()),\n","                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n","                ])\n","  sgd.fit(X_train, y_train)\n","  sgd_pred = sgd.predict(X_test)\n","  print(\"Linear Support Vector Machine\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, sgd_pred, target_names=my_tags,zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  # Logistic Regression\n","  from sklearn.linear_model import LogisticRegression\n","\n","  logreg = Pipeline([('vect', CountVectorizer()),\n","                  ('tfidf', TfidfTransformer()),\n","                  ('clf', LogisticRegression(n_jobs=2, C=1e5)),\n","                ])\n","  logreg.fit(X_train, y_train)\n","  logreg_pred = logreg.predict(X_test)\n","  print(\"Logistic Regression\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, logreg_pred,target_names=my_tags,zero_division=np.nan))\n","\n","  # Random Forest Classifier\n","  from sklearn.ensemble import RandomForestClassifier\n","\n","  forest = Pipeline([('vect', CountVectorizer()),\n","                  ('tfidf', TfidfTransformer()),\n","                  ('clf', RandomForestClassifier(max_depth=24, random_state=42)),\n","                ])\n","  forest.fit(X_train, y_train)\n","  forest_pred = forest.predict(X_test)\n","  print(\"Random Forest Classifier\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, forest_pred,target_names=my_tags,zero_division=np.nan))"],"metadata":{"id":"6hulTHzFiDmN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_text(\"malayalam\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IIR2Ul-5iy_9","executionInfo":{"status":"ok","timestamp":1736529070828,"user_tz":300,"elapsed":2423,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"b76b8021-bf26-40b5-e315-f37919fadf96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.82      0.64      0.72        42\n","           N        nan      0.00      0.00        23\n","           P       0.58      1.00      0.73       107\n","           R       1.00      0.07      0.14        27\n","           G        nan      0.00      0.00        22\n","\n","    accuracy                           0.62       221\n","   macro avg       0.80      0.34      0.32       221\n","weighted avg       0.70      0.62      0.51       221\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.72      0.90      0.80        42\n","           N       0.50      0.13      0.21        23\n","           P       0.78      0.93      0.85       107\n","           R       0.45      0.33      0.38        27\n","           G       0.57      0.36      0.44        22\n","\n","    accuracy                           0.71       221\n","   macro avg       0.60      0.53      0.54       221\n","weighted avg       0.68      0.71      0.68       221\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.67      0.69      0.68        42\n","           N       0.35      0.35      0.35        23\n","           P       0.79      0.83      0.81       107\n","           R       0.40      0.44      0.42        27\n","           G       0.62      0.36      0.46        22\n","\n","    accuracy                           0.66       221\n","   macro avg       0.57      0.54      0.54       221\n","weighted avg       0.66      0.66      0.66       221\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.71      0.69      0.70        42\n","           N        nan      0.00      0.00        23\n","           P       0.59      0.96      0.73       107\n","           R       0.67      0.07      0.13        27\n","           G       0.50      0.05      0.08        22\n","\n","    accuracy                           0.61       221\n","   macro avg       0.62      0.35      0.33       221\n","weighted avg       0.61      0.61      0.51       221\n","\n"]}]},{"cell_type":"code","source":["read_text(\"tamil\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_ZjqXAAjX_l","executionInfo":{"status":"ok","timestamp":1736529080097,"user_tz":300,"elapsed":848,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"2bcb9d8e-960e-4988-8395-b8c1c8debdb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C        nan      0.00      0.00        16\n","           N        nan      0.00      0.00        21\n","           P       0.53      1.00      0.70        69\n","           R        nan      0.00      0.00         7\n","           G        nan      0.00      0.00        16\n","\n","    accuracy                           0.53       129\n","   macro avg       0.53      0.20      0.14       129\n","weighted avg       0.53      0.53      0.37       129\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.22      0.12      0.16        16\n","           N       0.45      0.48      0.47        21\n","           P       0.80      0.83      0.81        69\n","           R       0.40      0.29      0.33         7\n","           G       0.45      0.62      0.53        16\n","\n","    accuracy                           0.63       129\n","   macro avg       0.47      0.47      0.46       129\n","weighted avg       0.61      0.63      0.61       129\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.09      0.06      0.07        16\n","           N       0.50      0.48      0.49        21\n","           P       0.72      0.81      0.76        69\n","           R       0.20      0.14      0.17         7\n","           G       0.40      0.38      0.39        16\n","\n","    accuracy                           0.57       129\n","   macro avg       0.38      0.37      0.38       129\n","weighted avg       0.54      0.57      0.55       129\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.50      0.06      0.11        16\n","           N       1.00      0.19      0.32        21\n","           P       0.57      1.00      0.72        69\n","           R        nan      0.00      0.00         7\n","           G       0.00      0.00      0.00        16\n","\n","    accuracy                           0.57       129\n","   macro avg       0.52      0.25      0.23       129\n","weighted avg       0.56      0.57      0.45       129\n","\n"]}]},{"cell_type":"code","source":["read_text(\"telugu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--IobFIOjYc8","executionInfo":{"status":"ok","timestamp":1736529109293,"user_tz":300,"elapsed":1058,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"a21fc498-5719-4656-aef6-83978adc9fb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.89      0.21      0.34        38\n","           N       0.67      0.22      0.33        27\n","           P       0.35      0.98      0.51        43\n","           R        nan      0.00      0.00        15\n","           G        nan      0.00      0.00        16\n","\n","    accuracy                           0.40       139\n","   macro avg       0.63      0.28      0.24       139\n","weighted avg       0.62      0.40      0.32       139\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.83      0.53      0.65        38\n","           N       0.55      0.63      0.59        27\n","           P       0.55      0.79      0.65        43\n","           R       0.57      0.27      0.36        15\n","           G       0.53      0.50      0.52        16\n","\n","    accuracy                           0.60       139\n","   macro avg       0.61      0.54      0.55       139\n","weighted avg       0.63      0.60      0.59       139\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.77      0.45      0.57        38\n","           N       0.47      0.59      0.52        27\n","           P       0.60      0.77      0.67        43\n","           R       0.45      0.33      0.38        15\n","           G       0.47      0.50      0.48        16\n","\n","    accuracy                           0.57       139\n","   macro avg       0.55      0.53      0.53       139\n","weighted avg       0.59      0.57      0.56       139\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.64      0.66      0.65        38\n","           N       0.62      0.37      0.47        27\n","           P       0.52      0.98      0.68        43\n","           R       0.50      0.07      0.12        15\n","           G       1.00      0.06      0.12        16\n","\n","    accuracy                           0.57       139\n","   macro avg       0.66      0.43      0.41       139\n","weighted avg       0.63      0.57      0.50       139\n","\n"]}]},{"cell_type":"markdown","source":["## Speech"],"metadata":{"id":"cqRh-kkftXhd"}},{"cell_type":"code","source":["def read_speech(lang):\n","\n","  import os\n","  import librosa\n","  import numpy as np\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.preprocessing import LabelEncoder\n","  from sklearn.ensemble import RandomForestClassifier\n","  from sklearn.metrics import classification_report\n","  from tqdm import tqdm\n","\n","  path = f\"!data/train/{lang}/audio\"\n","  dirs = os.listdir(path)\n","  dirs = sorted(dirs)\n","\n","  audio_data = []\n","  target_labels = []\n","\n","  for i in tqdm(dirs, total=len(dirs)):\n","\n","    file = os.path.join(path, i)\n","    y, sr = librosa.load(file)\n","    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","    spectrogram = spectrogram.T\n","\n","    label, details = i.split('_', 1)\n","\n","    audio_data.append(spectrogram)\n","    target_labels.append(label)\n","\n","  # Encode target labels\n","  label_encoder = LabelEncoder()\n","  encoded_labels = label_encoder.fit_transform(target_labels)\n","\n","  # Split data into training and testing sets\n","  X_train, X_test, y_train, y_test = train_test_split(audio_data, encoded_labels, test_size=0.25, random_state=42)\n","\n","  # Ensure all spectrograms have the same shape\n","  max_length = max([spec.shape[0] for spec in audio_data])\n","  X_train = [np.pad(spec, ((0, max_length - spec.shape[0]), (0, 0)), mode='constant') for spec in X_train]\n","  X_test = [np.pad(spec, ((0, max_length - spec.shape[0]), (0, 0)), mode='constant') for spec in X_test]\n","\n","  # Convert to NumPy arrays\n","  X_train = np.array(X_train)\n","  X_test = np.array(X_test)\n","\n","  # Convert the data to a flat 2D shape\n","  X_train_flat = X_train.reshape(X_train.shape[0], -1)\n","  X_test_flat = X_test.reshape(X_test.shape[0], -1)\n","\n","  my_tags = list(set(target_labels))\n","\n","  # Naive Bayes Classifier for Multinomial Models\n","  from sklearn.naive_bayes import MultinomialNB\n","  from sklearn.pipeline import Pipeline\n","  from sklearn.preprocessing import MinMaxScaler\n","\n","  nb = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n","  nb.fit(X_train_flat, y_train)\n","  nb_pred = nb.predict(X_test_flat)\n","  print(\"Naive Bayes Classifier for Multinomial Models\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, nb_pred,target_names=my_tags,zero_division=0))\n","  print(\"\\n\")\n","\n","  # Linear Support Vector Machine\n","  from sklearn.linear_model import SGDClassifier\n","\n","  sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n","  sgd.fit(X_train_flat, y_train)\n","  sgd_pred = sgd.predict(X_test_flat)\n","  print(\"Linear Support Vector Machine\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, sgd_pred, target_names=my_tags,zero_division=0))\n","  print(\"\\n\")\n","\n","  # Logistic Regression\n","  from sklearn.linear_model import LogisticRegression\n","\n","  logreg = LogisticRegression(n_jobs=2, C=1e5)\n","  logreg.fit(X_train_flat, y_train)\n","  logreg_pred = logreg.predict(X_test_flat)\n","  print(\"Logistic Regression\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, logreg_pred,target_names=my_tags))\n","  print(\"\\n\")\n","\n","  # Random Forest Classifier\n","  from sklearn.ensemble import RandomForestClassifier\n","\n","  forest = RandomForestClassifier(max_depth=24, random_state=42)\n","  forest.fit(X_train_flat, y_train)\n","  forest_pred = forest.predict(X_test_flat)\n","  print(\"Random Forest Classifier\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, forest_pred,target_names=my_tags))"],"metadata":{"id":"Xgu1sqh6BOn-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_speech(\"malayalam\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1pXqHnBCacE","executionInfo":{"status":"ok","timestamp":1736530024418,"user_tz":300,"elapsed":75118,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"89bb3782-5109-4b74-c5f3-ea93f1a0a170"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 883/883 [00:46<00:00, 19.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.78      0.65      0.71       125\n","           H       0.62      0.76      0.69        96\n","\n","    accuracy                           0.70       221\n","   macro avg       0.70      0.70      0.70       221\n","weighted avg       0.71      0.70      0.70       221\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.66      0.96      0.78       125\n","           H       0.87      0.34      0.49        96\n","\n","    accuracy                           0.69       221\n","   macro avg       0.76      0.65      0.64       221\n","weighted avg       0.75      0.69      0.65       221\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.92      0.91      0.92       125\n","           H       0.89      0.90      0.89        96\n","\n","    accuracy                           0.90       221\n","   macro avg       0.90      0.90      0.90       221\n","weighted avg       0.91      0.90      0.91       221\n","\n","\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.92      0.92      0.92       125\n","           H       0.90      0.90      0.90        96\n","\n","    accuracy                           0.91       221\n","   macro avg       0.91      0.91      0.91       221\n","weighted avg       0.91      0.91      0.91       221\n","\n"]}]},{"cell_type":"code","source":["read_speech(\"tamil\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7002b1jDj74","executionInfo":{"status":"ok","timestamp":1736530101124,"user_tz":300,"elapsed":73256,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"a195f46a-b6e0-4b6e-b017-ed464eef4ccd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 509/509 [00:50<00:00, 10.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.50      0.48      0.49        54\n","           H       0.63      0.65      0.64        74\n","\n","    accuracy                           0.58       128\n","   macro avg       0.57      0.57      0.57       128\n","weighted avg       0.58      0.58      0.58       128\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.56      0.72      0.63        54\n","           H       0.74      0.58      0.65        74\n","\n","    accuracy                           0.64       128\n","   macro avg       0.65      0.65      0.64       128\n","weighted avg       0.66      0.64      0.64       128\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.63      0.59      0.61        54\n","           H       0.71      0.74      0.73        74\n","\n","    accuracy                           0.68       128\n","   macro avg       0.67      0.67      0.67       128\n","weighted avg       0.68      0.68      0.68       128\n","\n","\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.73      0.69      0.70        54\n","           H       0.78      0.81      0.79        74\n","\n","    accuracy                           0.76       128\n","   macro avg       0.75      0.75      0.75       128\n","weighted avg       0.76      0.76      0.76       128\n","\n"]}]},{"cell_type":"code","source":["read_speech(\"telugu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vDKPywkD96q","executionInfo":{"status":"ok","timestamp":1736530168601,"user_tz":300,"elapsed":62160,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"c06cb7ef-cf04-4991-c981-e519692ae982"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 551/551 [00:39<00:00, 14.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.75      0.72      0.74        94\n","           H       0.45      0.48      0.46        44\n","\n","    accuracy                           0.64       138\n","   macro avg       0.60      0.60      0.60       138\n","weighted avg       0.65      0.64      0.65       138\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.82      0.66      0.73        94\n","           H       0.48      0.68      0.57        44\n","\n","    accuracy                           0.67       138\n","   macro avg       0.65      0.67      0.65       138\n","weighted avg       0.71      0.67      0.68       138\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.77      0.86      0.81        94\n","           H       0.61      0.45      0.52        44\n","\n","    accuracy                           0.73       138\n","   macro avg       0.69      0.66      0.67       138\n","weighted avg       0.72      0.73      0.72       138\n","\n","\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","          NH       0.77      0.86      0.81        94\n","           H       0.61      0.45      0.52        44\n","\n","    accuracy                           0.73       138\n","   macro avg       0.69      0.66      0.67       138\n","weighted avg       0.72      0.73      0.72       138\n","\n"]}]},{"cell_type":"code","source":["def read_speech(lang):\n","\n","  import os\n","  import librosa\n","  import numpy as np\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.preprocessing import LabelEncoder\n","  from sklearn.ensemble import RandomForestClassifier\n","  from sklearn.metrics import classification_report\n","  from tqdm import tqdm\n","\n","  path = f\"!data/train/{lang}/audio\"\n","  dirs = os.listdir(path)\n","  dirs = sorted(dirs)\n","\n","  audio_data = []\n","  target_labels = []\n","\n","  for i in tqdm(dirs, total=len(dirs)):\n","\n","    file = os.path.join(path, i)\n","    y, sr = librosa.load(file)\n","    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","    spectrogram = spectrogram.T\n","\n","    label, language, number, class_label, details = i.split('_', 4)\n","\n","    audio_data.append(spectrogram)\n","    target_labels.append(class_label)\n","\n","  # Encode target labels\n","  label_encoder = LabelEncoder()\n","  encoded_labels = label_encoder.fit_transform(target_labels)\n","\n","  # Split data into training and testing sets\n","  X_train, X_test, y_train, y_test = train_test_split(audio_data, encoded_labels, test_size=0.25, random_state=42)\n","\n","  # Ensure all spectrograms have the same shape\n","  max_length = max([spec.shape[0] for spec in audio_data])\n","  X_train = [np.pad(spec, ((0, max_length - spec.shape[0]), (0, 0)), mode='constant') for spec in X_train]\n","  X_test = [np.pad(spec, ((0, max_length - spec.shape[0]), (0, 0)), mode='constant') for spec in X_test]\n","\n","  # Convert to NumPy arrays\n","  X_train = np.array(X_train)\n","  X_test = np.array(X_test)\n","\n","  # Convert the data to a flat 2D shape\n","  X_train_flat = X_train.reshape(X_train.shape[0], -1)\n","  X_test_flat = X_test.reshape(X_test.shape[0], -1)\n","\n","  my_tags = list(set(target_labels))\n","\n","  # Naive Bayes Classifier for Multinomial Models\n","  from sklearn.naive_bayes import MultinomialNB\n","  from sklearn.pipeline import Pipeline\n","  from sklearn.preprocessing import MinMaxScaler\n","\n","  nb = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n","  nb.fit(X_train_flat, y_train)\n","  nb_pred = nb.predict(X_test_flat)\n","  print(\"Naive Bayes Classifier for Multinomial Models\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, nb_pred,target_names=my_tags,zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  # Linear Support Vector Machine\n","  from sklearn.linear_model import SGDClassifier\n","\n","  sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n","  sgd.fit(X_train_flat, y_train)\n","  sgd_pred = sgd.predict(X_test_flat)\n","  print(\"Linear Support Vector Machine\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, sgd_pred, target_names=my_tags,zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  # Logistic Regression\n","  from sklearn.linear_model import LogisticRegression\n","\n","  logreg = LogisticRegression(n_jobs=2, C=1e5)\n","  logreg.fit(X_train_flat, y_train)\n","  logreg_pred = logreg.predict(X_test_flat)\n","  print(\"Logistic Regression\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, logreg_pred,target_names=my_tags,zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  # Random Forest Classifier\n","  from sklearn.ensemble import RandomForestClassifier\n","\n","  forest = RandomForestClassifier(max_depth=24, random_state=42)\n","  forest.fit(X_train_flat, y_train)\n","  forest_pred = forest.predict(X_test_flat)\n","  print(\"Random Forest Classifier\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, forest_pred,target_names=my_tags,zero_division=np.nan))"],"metadata":{"id":"x1U-rEIBe70X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_speech(\"malayalam\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTQRWrmOfbyE","executionInfo":{"status":"ok","timestamp":1736530651162,"user_tz":300,"elapsed":145214,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"d731a01e-9061-40a2-9eeb-7d6be8aa8ac0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 883/883 [00:54<00:00, 16.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.41      0.48      0.44        42\n","           N       0.17      0.26      0.20        23\n","           P       0.68      0.74      0.71        96\n","           R       0.80      0.11      0.19        37\n","           G       0.15      0.17      0.16        23\n","\n","    accuracy                           0.48       221\n","   macro avg       0.44      0.35      0.34       221\n","weighted avg       0.54      0.48      0.46       221\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.69      0.79      0.73        42\n","           N       1.00      0.04      0.08        23\n","           P       0.84      0.94      0.89        96\n","           R       0.43      0.76      0.55        37\n","           G        nan      0.00      0.00        23\n","\n","    accuracy                           0.69       221\n","   macro avg       0.74      0.50      0.45       221\n","weighted avg       0.75      0.69      0.63       221\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.64      0.76      0.70        42\n","           N       0.32      0.30      0.31        23\n","           P       0.86      0.94      0.90        96\n","           R       0.66      0.51      0.58        37\n","           G       0.27      0.17      0.21        23\n","\n","    accuracy                           0.69       221\n","   macro avg       0.55      0.54      0.54       221\n","weighted avg       0.66      0.69      0.67       221\n","\n","\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.58      0.69      0.63        42\n","           N       0.50      0.09      0.15        23\n","           P       0.74      0.99      0.84        96\n","           R       0.62      0.35      0.45        37\n","           G       0.41      0.30      0.35        23\n","\n","    accuracy                           0.66       221\n","   macro avg       0.57      0.48      0.48       221\n","weighted avg       0.63      0.66      0.61       221\n","\n"]}]},{"cell_type":"code","source":["read_speech(\"tamil\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v3iIYL7OgwIo","executionInfo":{"status":"ok","timestamp":1736530798110,"user_tz":300,"elapsed":115600,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"7abf7c48-90c0-4618-fe5b-5e85fe0ee293"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 509/509 [00:48<00:00, 10.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C        nan      0.00      0.00        12\n","           N       0.12      0.19      0.15        21\n","           P       0.62      0.78      0.69        74\n","           R       0.50      0.12      0.20         8\n","           G       0.00      0.00      0.00        13\n","\n","    accuracy                           0.49       128\n","   macro avg       0.31      0.22      0.21       128\n","weighted avg       0.45      0.49      0.44       128\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C        nan      0.00      0.00        12\n","           N        nan      0.00      0.00        21\n","           P       0.59      0.99      0.74        74\n","           R        nan      0.00      0.00         8\n","           G       0.00      0.00      0.00        13\n","\n","    accuracy                           0.57       128\n","   macro avg       0.29      0.20      0.15       128\n","weighted avg       0.50      0.57      0.43       128\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.08      0.08      0.08        12\n","           N       0.46      0.29      0.35        21\n","           P       0.69      0.76      0.72        74\n","           R       0.50      0.38      0.43         8\n","           G       0.33      0.38      0.36        13\n","\n","    accuracy                           0.55       128\n","   macro avg       0.41      0.38      0.39       128\n","weighted avg       0.55      0.55      0.55       128\n","\n","\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.12      0.08      0.10        12\n","           N       0.38      0.14      0.21        21\n","           P       0.68      0.93      0.79        74\n","           R       1.00      0.12      0.22         8\n","           G       0.30      0.23      0.26        13\n","\n","    accuracy                           0.60       128\n","   macro avg       0.50      0.30      0.32       128\n","weighted avg       0.56      0.60      0.54       128\n","\n"]}]},{"cell_type":"code","source":["read_speech(\"telugu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsIWqWCzgw1X","executionInfo":{"status":"ok","timestamp":1736530288521,"user_tz":300,"elapsed":108234,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"ee9371f1-aec5-413f-8748-c0f584422b3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 551/551 [00:33<00:00, 16.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Naive Bayes Classifier for Multinomial Models\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.35      0.61      0.44        28\n","           N       0.30      0.09      0.14        33\n","           P       0.58      0.34      0.43        44\n","           R       0.21      0.64      0.32        14\n","           G       0.09      0.05      0.07        19\n","\n","    accuracy                           0.33       138\n","   macro avg       0.31      0.35      0.28       138\n","weighted avg       0.36      0.33      0.30       138\n","\n","\n","\n","Linear Support Vector Machine\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.43      0.82      0.56        28\n","           N       0.28      0.27      0.28        33\n","           P       0.52      0.55      0.53        44\n","           R       1.00      0.07      0.13        14\n","           G       0.40      0.11      0.17        19\n","\n","    accuracy                           0.43       138\n","   macro avg       0.53      0.36      0.33       138\n","weighted avg       0.48      0.43      0.39       138\n","\n","\n","\n","Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.46      0.68      0.55        28\n","           N       0.45      0.27      0.34        33\n","           P       0.53      0.45      0.49        44\n","           R       0.21      0.29      0.24        14\n","           G       0.25      0.26      0.26        19\n","\n","    accuracy                           0.41       138\n","   macro avg       0.38      0.39      0.38       138\n","weighted avg       0.43      0.41      0.41       138\n","\n","\n","\n","Random Forest Classifier\n","\n","\n","              precision    recall  f1-score   support\n","\n","           C       0.38      0.71      0.50        28\n","           N       0.55      0.18      0.27        33\n","           P       0.48      0.68      0.57        44\n","           R       0.38      0.36      0.37        14\n","           G        nan      0.00      0.00        19\n","\n","    accuracy                           0.44       138\n","   macro avg       0.45      0.39      0.34       138\n","weighted avg       0.47      0.44      0.38       138\n","\n"]}]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"87n9ODgzLFzk"}},{"cell_type":"code","source":["def read_test(lang, code):\n","\n","  import os\n","  import librosa\n","  import numpy as np\n","  import pandas as pd\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.ensemble import RandomForestClassifier\n","  from sklearn.metrics import classification_report\n","  from tqdm import tqdm\n","\n","  path = f\"!data/train/{lang}/audio\"\n","  dirs = os.listdir(path)\n","\n","  audio_data = []\n","  target_labels = []\n","\n","  # Convert .wav files to spectrogram\n","  for i in tqdm(dirs, total=len(dirs)):\n","\n","    file = os.path.join(path, i)\n","    y, sr = librosa.load(file)\n","\n","    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","    spectrogram = spectrogram.T\n","\n","    label, language, number, class_label, details = i.split('_', 4)\n","\n","    audio_data.append(spectrogram)\n","    target_labels.append(class_label)\n","\n","  mapping = {'N': 0, 'C': 1, 'P': 2, 'R': 3, 'G': 4}\n","  numeric_list = [mapping[label] for label in target_labels]\n","\n","  # Split data into training and testing sets\n","  X_train, X_test, y_train, y_test = train_test_split(audio_data, numeric_list, test_size=0.25, random_state=42)\n","\n","  # Ensure all spectrograms have the same shape\n","  max_length = max([spec.shape[0] for spec in audio_data])\n","  X_train = [np.pad(spec, ((0, max_length - spec.shape[0]), (0, 0)), mode='constant') for spec in X_train]\n","  X_test = [np.pad(spec, ((0, max_length - spec.shape[0]), (0, 0)), mode='constant') for spec in X_test]\n","\n","  # Convert to NumPy arrays\n","  X_train = np.array(X_train)\n","  X_test = np.array(X_test)\n","\n","  # Convert the data to a flat 2D shape\n","  X_train_flat = X_train.reshape(X_train.shape[0], -1)\n","  X_test_flat = X_test.reshape(X_test.shape[0], -1)\n","\n","  my_tags = ['N', 'C', 'P', 'R', 'G']\n","\n","  # Logistic Regression\n","  from sklearn.linear_model import LogisticRegression\n","\n","  logreg = LogisticRegression(n_jobs=2, C=1e5)\n","  logreg.fit(X_train_flat, y_train)\n","  logreg_pred = logreg.predict(X_test_flat)\n","  print(\"Logistic Regression\")\n","  print(\"\\n\")\n","  print(classification_report(y_test, logreg_pred, target_names=my_tags, zero_division=np.nan))\n","  print(\"\\n\")\n","\n","  test_path = f\"!data/test/{lang}/audio\"\n","  dirs = os.listdir(test_path)\n","\n","  test_audio = []\n","\n","  for i in tqdm(dirs, total=len(dirs)):\n","\n","    file = os.path.join(test_path, i)\n","    y, sr = librosa.load(file)\n","    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","    spectrogram = spectrogram.T\n","\n","    test_audio.append(spectrogram)\n","\n","  # Ensure all spectrograms have the same shape\n","  max_length = max([spec.shape[0] for spec in audio_data])\n","  test_data = [np.pad(spec, ((0, max_length - spec.shape[0]), (0, 0)), mode='constant') for spec in test_audio]\n","\n","  # Convert to NumPy arrays\n","  test_data = np.array(test_data)\n","\n","  # Convert the data to a flat 2D shape\n","  test_data_flat = test_data.reshape(test_data.shape[0], -1)\n","\n","  # Run model on unseen test data\n","  logreg_test = logreg.predict(test_data_flat)\n","\n","  # Map Short Class Labels\n","  mapping = {0: 'N', 1: 'C', 2: 'P', 3: 'R', 4: 'G'}\n","  test_pred = [mapping[label] for label in logreg_test]\n","  test_pred = list(test_pred)\n","\n","  # Create Dataframe\n","  output = pd.DataFrame(list(zip(test_pred, dirs)), columns=['Class Label Short', 'File Name'])\n","  output[['File Name', 'Drop']] = output['File Name'].str.split('.', expand=True)\n","  output.drop('Drop', axis=1, inplace=True)\n","\n","  # Read output file\n","  file_text = f\"!data/test/{lang}/text/{code}-AT-test.xlsx\"\n","  text = pd.read_excel(open(file_text,'rb'))\n","\n","  # Rename file name if Telugu\n","  if lang == \"telugu\":\n","    output.rename(columns={'File Name': 'File_Name'}, inplace=True)\n","    df = output.merge(text, on='File_Name')\n","  else:\n","    df = output.merge(text, on='File Name')\n","\n","  # Write .tsv file\n","  file_name = f'cantnlp/cantnlp_multimodal_{lang}.tsv'\n","  df.to_csv(file_name, sep=\"\\t\", encoding='utf-8', index=False)"],"metadata":{"id":"KM28cD27LFQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_test('malayalam', 'ML')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKYO1FznQknB","executionInfo":{"status":"ok","timestamp":1736565377920,"user_tz":300,"elapsed":132345,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"13843970-c5f5-41b7-d831-1bcdce878f7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 883/883 [00:51<00:00, 16.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           N       0.89      0.96      0.92        98\n","           C       0.67      0.60      0.63        53\n","           P       0.53      0.61      0.57        28\n","           R       0.38      0.25      0.30        20\n","           G       0.27      0.27      0.27        22\n","\n","    accuracy                           0.70       221\n","   macro avg       0.55      0.54      0.54       221\n","weighted avg       0.68      0.70      0.69       221\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:04<00:00, 11.70it/s]\n"]}]},{"cell_type":"code","source":["read_test('tamil', 'TA')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlDdozttQvfx","executionInfo":{"status":"ok","timestamp":1736565520148,"user_tz":300,"elapsed":128978,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"4ad5746a-fa4d-44c5-a3ec-7ab3a6d1cf96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 509/509 [01:09<00:00,  7.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           N       0.77      0.79      0.78        72\n","           C       0.18      0.11      0.13        19\n","           P       0.33      0.29      0.31         7\n","           R       0.41      0.47      0.44        15\n","           G       0.30      0.40      0.34        15\n","\n","    accuracy                           0.58       128\n","   macro avg       0.40      0.41      0.40       128\n","weighted avg       0.56      0.58      0.57       128\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:04<00:00, 12.13it/s]\n"]}]},{"cell_type":"code","source":["read_test('telugu', 'TE')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEU0RaElLiOQ","executionInfo":{"status":"ok","timestamp":1736565633024,"user_tz":300,"elapsed":104916,"user":{"displayName":"Sidney Wong","userId":"08044802347198242788"}},"outputId":"7bf7a46e-2112-4c2a-e26c-2ac55928102d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 551/551 [00:38<00:00, 14.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression\n","\n","\n","              precision    recall  f1-score   support\n","\n","           N       0.64      0.76      0.69        46\n","           C       0.69      0.69      0.69        35\n","           P       0.36      0.42      0.38        12\n","           R       0.21      0.15      0.18        20\n","           G       0.45      0.36      0.40        25\n","\n","    accuracy                           0.55       138\n","   macro avg       0.47      0.47      0.47       138\n","weighted avg       0.53      0.55      0.54       138\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:02<00:00, 19.66it/s]\n"]}]}]}